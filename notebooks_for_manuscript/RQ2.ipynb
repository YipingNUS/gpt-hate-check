{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from time import monotonic\n",
    "import numpy as np \n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1c272",
   "metadata": {},
   "source": [
    "# HS Dataset Analysis\n",
    "\n",
    "Compare dataset properties (ChatGPT vs. HateCheck)\n",
    "\n",
    "- Perplexity\n",
    "- Diversity\n",
    "- Topic distribution\n",
    "- Following the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eaad1d",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab216b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud config\n",
    "data_root = Path(\".\")\n",
    "hatecheck_config = {\"path\": data_root/\"hatecheck-data/test_suite_cases.csv\",\n",
    "                    \"text_col\": \"test_case\",\n",
    "                    \"func_col\": \"functionality\",\n",
    "                    \"excluded_func_prefix\": \"spell_\"\n",
    "}\n",
    "\n",
    "gpt_hate_config = {\"path\": Path(\"gpt-dataset\"),\n",
    "                    \"text_col\": \"message\",\n",
    "                    \"func_col\": \"functionality\",\n",
    "                    \"excluded_func_prefix\": \"F25-29: Spelling variation\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b086e16-5057-4603-969f-af8260f9cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local config\n",
    "data_root = Path(\"../datasets/\")\n",
    "hatecheck_config = {\"path\": data_root/\"hatecheck-data/test_suite_cases.csv\",\n",
    "                    \"text_col\": \"test_case\",\n",
    "                    \"func_col\": \"functionality\",\n",
    "                    \"excluded_func_prefix\": \"spell_\"\n",
    "}\n",
    "\n",
    "gpt_hate_config = {\"path\": Path(\"../nli_hypothesis_test/output\"),\n",
    "                    \"text_col\": \"message\",\n",
    "                    \"func_col\": \"functionality\",\n",
    "                    \"excluded_func_prefix\": \"F25-29: Spelling variation\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"gpt\"  # TODO: change me. {\"hatecheck\", \"gpt\"}\n",
    "\n",
    "if dataset == \"hatecheck\":\n",
    "    config = hatecheck_config\n",
    "elif dataset == \"gpt\":\n",
    "    config = gpt_hate_config\n",
    "else:\n",
    "    print(f\"Unidentified dataset: {dataset}\")\n",
    "    config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"hatecheck\":\n",
    "    df = pd.read_csv(config['path'], encoding='utf-8', low_memory=False)\n",
    "    print(f\"Loaded {len(df)} examples.\")\n",
    "elif dataset == \"gpt\":\n",
    "    dfs = []\n",
    "    for f in config[\"path\"].glob(\"dataset_*.csv\"):\n",
    "        dfs.append(pd.read_csv(f))\n",
    "    df = pd.concat(dfs)\n",
    "    \n",
    "    # filter by pass NLI test\n",
    "    print(f\"Before NLI test: {len(df)} examples.\")\n",
    "    df = df[df[\"nli_pass_test\"]==1]\n",
    "    print(f\"After NLI test: {len(df)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cccf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before dropping duplicates {len(df)} entries.\")\n",
    "df.drop_duplicates(subset=[config['text_col']], keep='first', inplace=True, ignore_index=True)\n",
    "print(f\"After dropping duplicates {len(df)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the test cases involving spelling errors since they'll influnce the stats calculation\n",
    "df  = df.loc[~df[config['func_col']].str.startswith(config[\"excluded_func_prefix\"], na=False)]\n",
    "print(f\"Remaining {len(df)} examples after excluding spelling errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5876109",
   "metadata": {},
   "source": [
    "## 2. Calculate the stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41b580",
   "metadata": {},
   "source": [
    "### 2.1 Self-BLEU to calculate the diversity\n",
    "\n",
    "- Use `Self-BLEU-1` and `Self-BLEU-2`\n",
    "- The lower the more diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = df[config['text_col']].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59837b5-0e36-473b-87ae-fbe62593efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_tokenized = [[word.lower() for word in word_tokenize(str(candidate)) if word.isalpha()] for candidate in candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b7f86-9b5f-40a5-9702-1fd681804d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu2_scores, bleu3_scores, bleu4_scores = [], [], []\n",
    "for i in trange(len(candidate_tokenized)):\n",
    "    references = candidate_tokenized.copy()\n",
    "    references.pop(i)\n",
    "    b2, b3, b4 = sentence_bleu(references=references, hypothesis=candidate_tokenized[i], \n",
    "        weights=[(1./2., 1./2.), (1./3., 1./3., 1./3.),(1./4., 1./4., 1./4., 1./4.)])\n",
    "    bleu2_scores.append(b2)\n",
    "    bleu3_scores.append(b3)\n",
    "    bleu4_scores.append(b4)\n",
    "sum(bleu2_scores)/len(bleu2_scores), sum(bleu3_scores)/len(bleu3_scores), sum(bleu4_scores)/len(bleu4_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafac1e3-0e47-4ab0-ae57-a67be6e1c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp cell\n",
    "i = 0\n",
    "references = candidate_tokenized[:i] + candidate_tokenized[i+1 :]\n",
    "sentence_bleu(references=references, hypothesis=candidate_tokenized[i], \n",
    "                  weights=[(1., 0), (1./2., 1./2.)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f05b5-f2ea-47b5-ad02-0f6665d8618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp cell\n",
    "bleu1_scores, bleu2_scores = [], []\n",
    "for reference in references:\n",
    "    b1, b2 = sentence_bleu(references=[reference], hypothesis=candidate_tokenized[i], \n",
    "                  weights=[(1., 0), (1./2., 1./2.)])\n",
    "    bleu1_scores.append(b1)\n",
    "    bleu2_scores.append(b2)\n",
    "sum(bleu1_scores)/len(bleu1_scores), sum(bleu2_scores)/len(bleu2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790d377-93e5-44bc-85d7-8edb6ee04fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb2s, sb3s, sb4s = [], [], []\n",
    "if dataset == \"gpt\":\n",
    "    for i in range(10):\n",
    "        print(f\"Run {i}:\")\n",
    "        df_sample = df.sample(n=2968, random_state=i)\n",
    "        \n",
    "        candidates = df_sample[config['text_col']].tolist()\n",
    "        candidate_tokenized = [[word.lower() for word in word_tokenize(str(candidate)) if word.isalpha()] for candidate in candidates]\n",
    "\n",
    "        bleu2_scores, bleu3_scores, bleu4_scores = [], [], []\n",
    "        for j in trange(len(candidate_tokenized)):\n",
    "            references = candidate_tokenized.copy()\n",
    "            references.pop(j)\n",
    "            b2, b3, b4 = sentence_bleu(references=references, hypothesis=candidate_tokenized[j], \n",
    "                weights=[(1./2., 1./2.), (1./3., 1./3., 1./3.),(1./4., 1./4., 1./4., 1./4.)])\n",
    "            bleu2_scores.append(b2)\n",
    "            bleu3_scores.append(b3)\n",
    "            bleu4_scores.append(b4)\n",
    "        bleu_2 = sum(bleu2_scores)/len(bleu2_scores)\n",
    "        bleu_3 = sum(bleu3_scores)/len(bleu3_scores)\n",
    "        bleu_4 = sum(bleu4_scores)/len(bleu4_scores)\n",
    "        print(f\"bleu-2={bleu_2}, bleu-3={bleu_3}, bleu-4={bleu_4}\")\n",
    "        sb2s.append(bleu_2)\n",
    "        sb3s.append(bleu_3)\n",
    "        sb4s.append(bleu_4)\n",
    "        with open(f\"run_{i}.log\", \"w+\") as f:\n",
    "            f.write(f\"bleu-2={bleu_2}, bleu-3={bleu_3}, bleu-4={bleu_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6f02f-acac-493f-8713-529480b0e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sb2s), np.std(sb2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070aeff7-7939-49d3-8308-83c45dcd61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sb3s), np.std(sb3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a6952-6cc8-4628-9c80-4b8b00069adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sb4s), np.std(sb4s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407653c8-7ee1-4967-9552-36c15f585532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "stats.ttest_1samp(sb2s, popmean=0.937088016679527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf9ef4-3a3f-4471-a63b-a44decad59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(sb3s, popmean=0.8627437449674054)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b896f-cd36-4a12-9893-021b248faf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(sb4s, popmean=0.7611379109893396)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715924b",
   "metadata": {},
   "source": [
    "## 2.2 Perplexity to calculate fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "ppl_model_name = \"gpt2-large\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_model = GPT2LMHeadModel.from_pretrained(ppl_model_name).to(device)\n",
    "ppl_tokenizer = GPT2TokenizerFast.from_pretrained(ppl_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppl_score(texts, verbose=False):\n",
    "    \"\"\" Calculate the negative log likelihood score.\n",
    "    Since we care only about the rank, it's no difference from the perplexity\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    nnl_loss = list()\n",
    "    for i in trange(len(texts)):\n",
    "        encoded_input = ppl_tokenizer(texts[i], return_tensors='pt').to(device)\n",
    "        target_ids = encoded_input['input_ids'].clone()\n",
    "        with torch.no_grad():\n",
    "            outputs = ppl_model(encoded_input['input_ids'], labels=target_ids)\n",
    "            nnl_loss.append(outputs['loss'].item())\n",
    "    return nnl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ed191",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = df[config['text_col']].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnls = ppl_score(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = torch.exp(torch.Tensor(nnls).mean())\n",
    "print(f\"Perplexity score: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62efafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppls = []\n",
    "if dataset == \"gpt\":\n",
    "    for i in range(10):\n",
    "        print(f\"Run {i}:\")\n",
    "        start_time = monotonic()\n",
    "        \n",
    "        df_sample = df.sample(n=2968, random_state=i)\n",
    "        \n",
    "        candidates = df_sample[config['text_col']].tolist()\n",
    "        nnls = ppl_score(candidates)\n",
    "        ppl = torch.exp(torch.Tensor(nnls).mean())\n",
    "        print(f\"Perplexity score: {ppl}\")\n",
    "        ppls.append(ppl)\n",
    "        print(f\"Run time {monotonic() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d834a-6e85-4bcd-9c80-8395307c707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ppls), np.std(ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbd61c-b1f5-43d1-948a-dcdb521a1547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
